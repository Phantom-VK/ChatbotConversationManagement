{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9gPRrq8PGjJb",
        "b4_fU74jJ66f",
        "6uTVD5n3eVyy",
        "f7t-lgDqeZq2",
        "s4Cge2reedr-",
        "TxuCIaXwGg1J",
        "K3yTmfOHID7k",
        "3Yo043fbK0z5",
        "hd7l3d_nK4Rt",
        "IS6xIFOLZLc0",
        "p5cdHC6JfLDo",
        "k5dMfgn5nmpT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloads and Imports**"
      ],
      "metadata": {
        "id": "9gPRrq8PGjJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BD0QavPNFhcp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27db6d24-8110-4531-ffd0-c2b53e32f409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/331.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m327.7/331.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet requests openai jsonschema pydantic[email]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict, Literal"
      ],
      "metadata": {
        "id": "bXXbMbfVH7BS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurations and Model Instances**"
      ],
      "metadata": {
        "id": "b4_fU74jJ66f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHAT_MODEL = \"llama-3.3-70b-versatile\"\n",
        "STRUCTURED_OUTPUT_MODEL = \"openai/gpt-oss-20b\""
      ],
      "metadata": {
        "id": "C0MEJ6f1LM4M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key = userdata.get(\"GROQ_API_KEY\")\n",
        ")"
      ],
      "metadata": {
        "id": "AyNdxjW7J-Dt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: Managing Conversation History with Summarization**\n",
        "\n",
        "- Message History Tracking\n",
        "- Summarization\n",
        "- Truncation"
      ],
      "metadata": {
        "id": "1ctXTVPmP8Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MESSAGE_HISTORY : List[dict] = []\n",
        "SUMMARY = \"\""
      ],
      "metadata": {
        "id": "fUIAOVyVqQ90"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to add messages"
      ],
      "metadata": {
        "id": "6uTVD5n3eVyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarization_interval,  The interval / turn count after which history summarization will happen\n",
        "turn_count = 0  # Variable to track turn\n",
        "def add_message(message:dict, summarization_interval:int = 8 ):\n",
        "    global turn_count\n",
        "    # Add message to history and increment turn\n",
        "    MESSAGE_HISTORY.append(message)\n",
        "    turn_count += 1\n",
        "\n",
        "    print(f\"Turn {turn_count}: Added {message}\")\n",
        "\n",
        "    # Check if periodic summarization is needed\n",
        "    if turn_count % summarization_interval == 0:\n",
        "      print(f\"Turn count {turn_count}, output: {turn_count % summarization_interval}\")\n",
        "      periodic_summarization()"
      ],
      "metadata": {
        "id": "ydfk_B0OP_br"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## functionm to create summary"
      ],
      "metadata": {
        "id": "f7t-lgDqeZq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(messages):\n",
        "\n",
        "    if not MESSAGE_HISTORY:\n",
        "        print(\"No messages to summarize.\")\n",
        "        return\n",
        "\n",
        "    # Convert messsage history which is in dictionary format, to a string\n",
        "    conversation_text = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in MESSAGE_HISTORY])\n",
        "\n",
        "    summary_prompt = f\"\"\"\n",
        "        Please provide a concise summary of the following conversation, in the third person point of view. Focus on:\n",
        "        1. Main topics discussed\n",
        "        2. Key decisions or outcomes\n",
        "        3. Important information exchanged\n",
        "        4. Overall context and flow\n",
        "\n",
        "        Conversation:\n",
        "        {conversation_text}\n",
        "\n",
        "        Summary:\"\"\"\n",
        "    try:\n",
        "        chat_bot_response = client.chat.completions.create(\n",
        "          model=CHAT_MODEL,\n",
        "            messages=[{\"role\":\"user\", \"content\":conversation_text}],\n",
        "            max_tokens=150,\n",
        "            temperature=0.3)\n",
        "\n",
        "        summary = chat_bot_response.choices[0].message.content.strip()\n",
        "\n",
        "        print(f\"‚úÖ Generated summary ({len(summary)} characters)\")\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating summary: {e}\")\n",
        "        return \"Summary generation failed\""
      ],
      "metadata": {
        "id": "P_io4vKcWO6E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to create periodic summary"
      ],
      "metadata": {
        "id": "s4Cge2reedr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def periodic_summarization():\n",
        "        global MESSAGE_HISTORY\n",
        "        global SUMMARY\n",
        "\n",
        "        # Only generate a new summary if there's enough new conversation\n",
        "        if len(MESSAGE_HISTORY) >= 2:\n",
        "            current_summary = generate_summary(MESSAGE_HISTORY)\n",
        "\n",
        "            if SUMMARY:\n",
        "                # Combine with previous summary\n",
        "                combined_prompt = f\"\"\"\n",
        "                Previous summary: {SUMMARY}\n",
        "\n",
        "                New conversation summary: {current_summary}\n",
        "\n",
        "                Please create a comprehensive summary that combines both:\"\"\"\n",
        "\n",
        "                try:\n",
        "                    response = client.chat.completions.create(\n",
        "                        model=CHAT_MODEL,\n",
        "                        messages=[{\"role\": \"user\", \"content\": combined_prompt}],\n",
        "                        max_tokens= 300,\n",
        "                        temperature=0.3\n",
        "                    )\n",
        "\n",
        "                    SUMMARY = response.choices[0].message.content.strip()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error combining summaries: {e}\")\n",
        "                    SUMMARY = current_summary\n",
        "            else:\n",
        "                SUMMARY = current_summary\n",
        "\n",
        "            # Replace history with summary\n",
        "            MESSAGE_HISTORY = [\n",
        "                {\"role\": \"system\", \"content\": f\"Previous conversation summary: {SUMMARY}\"}\n",
        "            ]\n",
        "\n",
        "            print(f\"Conversation summarized and history reset\")\n",
        "            print(f\"Current summary length: {len(SUMMARY)} characters\")"
      ],
      "metadata": {
        "id": "kkFgwJ8XezA2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "TxuCIaXwGg1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_words(text: str) -> int:\n",
        "    return len(text.split())\n",
        "\n",
        "def count_characters(messages: List[Dict]) -> int:\n",
        "    return sum(len(msg[\"content\"]) for msg in messages)\n",
        "\n",
        "def count_total_words(messages: List[Dict]) -> int:\n",
        "    return sum(count_words(msg[\"content\"]) for msg in messages)"
      ],
      "metadata": {
        "id": "LIu7myvAGgVv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to truncate conversation by turns"
      ],
      "metadata": {
        "id": "tnvwOw6CG5i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_turns, number of messages after which we want to truncate\n",
        "def truncate_by_turns(messages: List[Dict], max_turns:int) -> List[Dict]:\n",
        "  if max_turns and len(messages) > max_turns:\n",
        "      truncated = messages[-max_turns:]\n",
        "      print(f\"Truncated to last {max_turns} messages\")\n",
        "      return truncated\n",
        "  return messages"
      ],
      "metadata": {
        "id": "pE0WOBZKHAua"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Truncate by length"
      ],
      "metadata": {
        "id": "K3yTmfOHID7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_characters\n",
        "# max_words\n",
        "def truncate_by_length(messages: List[Dict], max_characters:int, max_words:int) -> List[Dict]:\n",
        "    if not (max_characters or max_words):\n",
        "        return messages\n",
        "\n",
        "    result = []\n",
        "    current_chars = 0\n",
        "    current_words = 0\n",
        "\n",
        "    # Start from the end to keep most recent messages\n",
        "    for message in reversed(messages):\n",
        "        msg_chars = len(message[\"content\"])\n",
        "        msg_words = count_words(message[\"content\"])\n",
        "\n",
        "        # Check if adding this message would exceed limits\n",
        "        if (max_characters and current_chars + msg_chars > max_characters) or (max_words and current_words + msg_words > max_words):\n",
        "            break\n",
        "\n",
        "        result.insert(0, message)\n",
        "        current_chars += msg_chars\n",
        "        current_words += msg_words\n",
        "\n",
        "    if len(result) < len(messages):\n",
        "        removed = len(messages) - len(result)\n",
        "        print(f\"üìù Truncated {removed} messages due to length constraints\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "7bbJJTIcIGaL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test Conversation Manager**"
      ],
      "metadata": {
        "id": "p4pUiT_Sev_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Conversation with LLM, test summarization"
      ],
      "metadata": {
        "id": "3Yo043fbK0z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start chat with LLM\n",
        "turn_count = 0 # reset turn count\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    add_message({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    try:\n",
        "        chat_bot_response = client.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=MESSAGE_HISTORY\n",
        "        )\n",
        "        response_content = chat_bot_response.choices[0].message.content\n",
        "        add_message({\"role\": \"assistant\", \"content\": response_content})\n",
        "\n",
        "        print(f\"Bot: {response_content}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "kc8J6W-8R7-g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "b0e894cf-2a24-496c-cb22-45e35036c336"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hello\n",
            "Turn 1: Added {'role': 'user', 'content': 'Hello'}\n",
            "Turn 2: Added {'role': 'assistant', 'content': 'Hello. Is there something I can help you with or would you like to chat?'}\n",
            "Bot: Hello. Is there something I can help you with or would you like to chat?\n",
            "User: What is a context window,  tell me in one words\n",
            "Turn 3: Added {'role': 'user', 'content': 'What is a context window,  tell me in one words'}\n",
            "Turn 4: Added {'role': 'assistant', 'content': 'Memory.'}\n",
            "Bot: Memory.\n",
            "User: Can a prompt be considered in  context window?\n",
            "Turn 5: Added {'role': 'user', 'content': 'Can a prompt be considered in  context window?'}\n",
            "Turn 6: Added {'role': 'assistant', 'content': 'Yes.'}\n",
            "Bot: Yes.\n",
            "User: Nice, on how many GPUs you weas trained?\n",
            "Turn 7: Added {'role': 'user', 'content': 'Nice, on how many GPUs you weas trained?'}\n",
            "Turn 8: Added {'role': 'assistant', 'content': '256-512 GPUs.'}\n",
            "Turn count 8, output: 0\n",
            "‚úÖ Generated summary (474 characters)\n",
            "Conversation summarized and history reset\n",
            "Current summary length: 1001 characters\n",
            "Bot: 256-512 GPUs.\n",
            "User: NIce, bye\n",
            "Turn 9: Added {'role': 'user', 'content': 'NIce, bye'}\n",
            "Turn 10: Added {'role': 'assistant', 'content': \"Goodbye. It was nice chatting with you, even if it was brief. If you have any questions or need assistance in the future, don't hesitate to reach out. Have a great day!\"}\n",
            "Bot: Goodbye. It was nice chatting with you, even if it was brief. If you have any questions or need assistance in the future, don't hesitate to reach out. Have a great day!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1035460742.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mturn_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# reset turn count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0madd_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SUMMARY"
      ],
      "metadata": {
        "id": "pbhqZoMjAM9Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "dce83023-1879-40a8-d699-b69dfe0a044b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here's a comprehensive summary that combines both:\\n\\nI'm a large language model with a maximum token capacity of approximately 2048 tokens and a context window of around 2048 tokens. My training process involved a massive corpus of text and a distributed computing architecture that utilized a cluster of 256-512 GPUs, as well as hundreds of GPUs in a broader sense. This robust infrastructure enabled me to learn from a vast amount of text data. Previously, we discussed my capabilities and training process, and now I'm ready to help with any new questions or topics you'd like to discuss. To recap, my context window allows me to consider up to 2048 tokens of previous conversation history, including prompts, conversation history, and any other relevant text, when generating a response. Feel free to ask me anything, and I'll do my best to provide helpful and accurate information. Whether you have follow-up questions about my capabilities or want to explore new subjects, I'm here to assist you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test truncation with test message jsons"
      ],
      "metadata": {
        "id": "hd7l3d_nK4Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_samples = [\n",
        "    (\"user\", \"Hi! I need advice on starting a food truck business.\"),\n",
        "    (\"assistant\", \"That's exciting! What type of cuisine are you considering for your food truck?\"),\n",
        "    (\"user\", \"I'm interested in serving gourmet tacos and fresh juices.\"),\n",
        "    (\"assistant\", \"Great choice! Have you picked a location or checked local food truck regulations?\"),\n",
        "    (\"user\", \"Not yet. Do you have tips on selecting a good spot?\"),\n",
        "    (\"assistant\", \"Absolutely. Look for areas with heavy foot traffic, local events, and nearby office parks.\"),\n",
        "    (\"user\", \"Thanks! What licenses do I need to get started?\"),\n",
        "    (\"assistant\", \"You'll need a mobile food vendor license, health permits, and possibly parking permits depending on your city.\"),\n",
        "    (\"user\", \"Got it. What about marketing ideas for my launch?\"),\n",
        "    (\"assistant\", \"Consider social media campaigns, local food festivals, and offering promo deals for first-time customers.\"),\n",
        "    (\"user\", \"That's helpful. How much investment do I need to get started?\"),\n",
        "    (\"assistant\", \"Typically, starting a food truck costs between $50,000 and $150,000, depending on equipment, customization, and permits.\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "Sv4vAGdYK_e0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear previous history for a clean test\n",
        "MESSAGE_HISTORY = []\n",
        "turn_count = 0\n",
        "SUMMARY = \"\"\n",
        "\n",
        "print(\"-------- Testing Truncation --------\")\n",
        "\n",
        "# fill message history with sample conversation\n",
        "for role, content in conversation_samples:\n",
        "    add_message({\"role\": role, \"content\": content})\n",
        "\n",
        "print(\"\\n-------- Original Message History --------\")\n",
        "for msg in MESSAGE_HISTORY:\n",
        "    print(f\"{msg['role']}: {msg['content']}\")\n",
        "print(f\"Total messages: {len(MESSAGE_HISTORY)}\")\n",
        "\n",
        "print(\"\\n-------- Truncating by Turns (max_turns=3) --------\")\n",
        "truncated_by_turns = truncate_by_turns(MESSAGE_HISTORY, max_turns=3)\n",
        "for msg in truncated_by_turns:\n",
        "    print(f\"{msg['role']}: {msg['content']}\")\n",
        "print(f\"Truncated messages: {len(truncated_by_turns)}\")\n",
        "\n",
        "\n",
        "print(\"\\n-------- Truncating by Length (max_characters=300, max_words=80) --------\")\n",
        "truncated_by_length = truncate_by_length(MESSAGE_HISTORY, max_characters=300, max_words=80)\n",
        "for msg in truncated_by_length:\n",
        "    print(f\"{msg['role']}: {msg['content']}\")\n",
        "print(f\"Truncated messages: {len(truncated_by_length)}\")\n",
        "\n",
        "print(\"\\n-------- Truncation Test Complete --------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E880Q_grLBj0",
        "outputId": "6f7208b8-55d5-4345-df2f-1b93e07fc1bf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------- Testing Truncation --------\n",
            "Turn 1: Added {'role': 'user', 'content': 'Hi! I need advice on starting a food truck business.'}\n",
            "Turn 2: Added {'role': 'assistant', 'content': \"That's exciting! What type of cuisine are you considering for your food truck?\"}\n",
            "Turn 3: Added {'role': 'user', 'content': \"I'm interested in serving gourmet tacos and fresh juices.\"}\n",
            "Turn 4: Added {'role': 'assistant', 'content': 'Great choice! Have you picked a location or checked local food truck regulations?'}\n",
            "Turn 5: Added {'role': 'user', 'content': 'Not yet. Do you have tips on selecting a good spot?'}\n",
            "Turn 6: Added {'role': 'assistant', 'content': 'Absolutely. Look for areas with heavy foot traffic, local events, and nearby office parks.'}\n",
            "Turn 7: Added {'role': 'user', 'content': 'Thanks! What licenses do I need to get started?'}\n",
            "Turn 8: Added {'role': 'assistant', 'content': \"You'll need a mobile food vendor license, health permits, and possibly parking permits depending on your city.\"}\n",
            "Turn count 8, output: 0\n",
            "‚úÖ Generated summary (756 characters)\n",
            "Conversation summarized and history reset\n",
            "Current summary length: 756 characters\n",
            "Turn 9: Added {'role': 'user', 'content': 'Got it. What about marketing ideas for my launch?'}\n",
            "Turn 10: Added {'role': 'assistant', 'content': 'Consider social media campaigns, local food festivals, and offering promo deals for first-time customers.'}\n",
            "Turn 11: Added {'role': 'user', 'content': \"That's helpful. How much investment do I need to get started?\"}\n",
            "Turn 12: Added {'role': 'assistant', 'content': 'Typically, starting a food truck costs between $50,000 and $150,000, depending on equipment, customization, and permits.'}\n",
            "\n",
            "-------- Original Message History --------\n",
            "system: Previous conversation summary: It's also a good idea to check with your local government for any specific requirements, such as food safety certifications or business registrations. Additionally, you may want to consider obtaining liability insurance to protect yourself and your business in case of accidents or food-borne illnesses. \n",
            "\n",
            "In terms of operations, have you thought about how you'll manage inventory, supplies, and waste disposal for your food truck? And do you have a plan for handling payments and point-of-sale transactions? \n",
            "\n",
            "It might also be helpful to develop a unique brand identity and marketing strategy to differentiate your gourmet taco and fresh juice business from others in the area. Do you have any ideas for your brand's name, logo, and social media presence?\n",
            "user: Got it. What about marketing ideas for my launch?\n",
            "assistant: Consider social media campaigns, local food festivals, and offering promo deals for first-time customers.\n",
            "user: That's helpful. How much investment do I need to get started?\n",
            "assistant: Typically, starting a food truck costs between $50,000 and $150,000, depending on equipment, customization, and permits.\n",
            "Total messages: 5\n",
            "\n",
            "-------- Truncating by Turns (max_turns=3) --------\n",
            "Truncated to last 3 messages\n",
            "assistant: Consider social media campaigns, local food festivals, and offering promo deals for first-time customers.\n",
            "user: That's helpful. How much investment do I need to get started?\n",
            "assistant: Typically, starting a food truck costs between $50,000 and $150,000, depending on equipment, customization, and permits.\n",
            "Truncated messages: 3\n",
            "\n",
            "-------- Truncating by Length (max_characters=300, max_words=80) --------\n",
            "üìù Truncated 2 messages due to length constraints\n",
            "assistant: Consider social media campaigns, local food festivals, and offering promo deals for first-time customers.\n",
            "user: That's helpful. How much investment do I need to get started?\n",
            "assistant: Typically, starting a food truck costs between $50,000 and $150,000, depending on equipment, customization, and permits.\n",
            "Truncated messages: 3\n",
            "\n",
            "-------- Truncation Test Complete --------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: JSON Schema Classification & Information Extraction**"
      ],
      "metadata": {
        "id": "CvA01nyZYASu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required JSON Schema and Prompts"
      ],
      "metadata": {
        "id": "IS6xIFOLZLc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, EmailStr\n",
        "from typing import List, Dict, Literal, Optional\n",
        "\n",
        "class UserInformationModel(BaseModel):\n",
        "    name: Optional[str] = None\n",
        "    email: Optional[EmailStr] = None\n",
        "    phone: Optional[str] = None\n",
        "    location: Optional[str] = None\n",
        "    age: Optional[int] = None\n",
        "    tech_stack: Optional[List[str]] = None"
      ],
      "metadata": {
        "id": "L8FwfVZBYGWX"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = system_prompt = \"\"\"You are an expert information extractor. Your task is to carefully analyze chat conversations and extract specific user information when available.\n",
        "\n",
        "\n",
        "FOLLOW THESE IMPORTANT GUIDELINES:\n",
        "1. Extract only information that is explicitly mentioned in the conversation\n",
        "2. Do not make assumptions or infer information not directly stated\n",
        "3. For names: Extract full names when provided\n",
        "4. For emails: Look for email addresses in standard formats, should contain '@'\n",
        "5. For phones: Extract any phone number mentioned (any format, should be 10 digit)\n",
        "6. For locations: Extract cities, states, addresses, or geographical references\n",
        "7. For age: Extract numerical age when mentioned\n",
        "8. For tech_stack: Extract a list of technical skills when mentioned\n",
        "\n",
        "Cross check all information before giving an output.\n",
        "\n",
        "If information is not clearly given, leave that field empty instead of guessing.\"\"\""
      ],
      "metadata": {
        "id": "l74lmwFgYdLi"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to extract information"
      ],
      "metadata": {
        "id": "p5cdHC6JfLDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_information(chat_conversation: str | List[Dict]) -> dict:\n",
        "\n",
        "  USER_PROMPT = f\"\"\"Please extract any available user information from this chat conversation:\n",
        "\n",
        "{chat_conversation}\n",
        "\n",
        "Use the extract_user_information function to provide the structured output.\"\"\"\n",
        "  try:\n",
        "      chat_bot_response = client.chat.completions.create(\n",
        "            model=STRUCTURED_OUTPUT_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\":\"user\", \"content\":USER_PROMPT},\n",
        "            ],\n",
        "            response_format =  {\n",
        "                \"type\": \"json_schema\",\n",
        "                \"json_schema\":{\n",
        "                    \"name\":\"user_information\",\n",
        "                    \"description\":\"User information extracted from the chat conversation\",\n",
        "                    \"schema\":UserInformationModel.model_json_schema()\n",
        "\n",
        "                } }\n",
        "        )\n",
        "      return chat_bot_response.choices[0].message.content\n",
        "  except Exception as e:\n",
        "    raise Exception(f\"Error extracting information: {e}\")"
      ],
      "metadata": {
        "id": "xt8sIJ5PfPEA"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Information Extraction"
      ],
      "metadata": {
        "id": "k5dMfgn5nmpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_CHATS = [\n",
        "    # Sample 1: Customer support chat with complete information, including tech stack\n",
        "    \"\"\"Customer: Hi, I'm having trouble with my order\n",
        "Agent: I'd be happy to help! Can I get your name and email?\n",
        "Customer: Sure, it's Sarah Johnson and my email is sarah.johnson@email.com\n",
        "Agent: Thank you Sarah. Can I also get your phone number and address?\n",
        "Customer: Yes, my phone is (555) 123-4567 and I live in Seattle, WA. I'm 28 years old by the way, does that matter for shipping?\n",
        "Agent: Thank you for all that information! Just to confirm, what tech do you use the most?\n",
        "Customer: Mostly Python and Django for my small business website.\n",
        "Agent: Let me look up your order.\"\"\",\n",
        "\n",
        "    # Sample 2: Registration chat with partial information & tech stack\n",
        "    \"\"\"User: I want to sign up for your newsletter\n",
        "Bot: Great! What's your email address?\n",
        "User: It's mike.chen@company.com\n",
        "Bot: Thanks! And your name?\n",
        "User: Mike Chen\n",
        "Bot: Any location preference for local events?\n",
        "User: I'm based in San Francisco.\n",
        "Bot: Do you use any specific tech tools or frameworks at work?\n",
        "User: Node.js and React.\n",
        "Bot: Perfect, you're all set!\"\"\",\n",
        "\n",
        "    # Sample 3: Survey chat with mixed information\n",
        "    \"\"\"Interviewer: Thank you for participating in our survey. Could you share some basic information?\n",
        "Participant: Of course\n",
        "Interviewer: What's your name and age?\n",
        "Participant: I'm Jennifer Davis, 35 years old\n",
        "Interviewer: Great! Do you have a preferred contact method?\n",
        "Participant: You can reach me at jennifer.davis.work@gmail.com or call me at 555-987-6543\n",
        "Interviewer: And which city are you located in?\n",
        "Participant: I live in Austin, Texas\n",
        "Interviewer: Do you work in any particular technology area?\n",
        "Participant: Yes, mainly with AWS, Java, and Docker.\n",
        "Interviewer: Perfect, that's all we need!\"\"\",\n",
        "\n",
        "    # Sample 4: Job application chat with multiple tech skills\n",
        "    \"\"\"Recruiter: Welcome, could you please tell me your name and how to reach you?\n",
        "Candidate: My name is Priya Sharma. You can email me at priya.sharma123@gmail.com.\n",
        "Recruiter: And your phone number and current location?\n",
        "Candidate: Sure, my phone is +91-9876543210, and I'm currently in Bengaluru.\n",
        "Recruiter: What's your age and primary tech stacks?\n",
        "Candidate: I'm 25. I primarily work with Vue.js, FastAPI, and PostgreSQL.\n",
        "Recruiter: Noted. Do you have experience with cloud platforms?\n",
        "Candidate: Yes, GCP and Azure are part of my daily workflow.\"\"\",\n",
        "\n",
        "    # Sample 5: Casual onboarding chat with partial information\n",
        "    \"\"\"Mentor: Hi! Welcome aboard. What's your full name?\n",
        "Newcomer: I'm Alex Martinez.\n",
        "Mentor: Great, Alex. You can share your contact details when you're ready.\n",
        "Newcomer: Sure! My email: alexmartinez@startup.io\n",
        "Mentor: Where are you joining from?\n",
        "Newcomer: I'm based in Boston. I usually build with Flutter and Firebase.\n",
        "Mentor: Awesome! Age is optional, but it helps with mentorship pairing.\n",
        "Newcomer: I'm 30.\n",
        "Mentor: Thanks, Alex. Let's start!\"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "pBTLdEbLgj1F"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Testing Information Extraction with Sample Chats ---\")\n",
        "\n",
        "for i, chat in enumerate(SAMPLE_CHATS):\n",
        "    print(f\"\\n--- Sample Chat {i+1} ---\")\n",
        "    print(chat)\n",
        "    try:\n",
        "        extracted_data_json = extract_information(chat)\n",
        "\n",
        "        extracted_data = json.loads(extracted_data_json)\n",
        "        validated_data = UserInformationModel.model_validate(extracted_data)\n",
        "        print(\"\\n‚úÖ Extracted and Validated Information:\")\n",
        "        print(validated_data.model_dump_json(indent=2))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error processing sample chat {i+1}: {e}\")\n",
        "\n",
        "print(\"\\n--- Information Extraction Test Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhF8SSq5nWI3",
        "outputId": "55c132f4-1c40-4988-8db2-54f67aa26e2c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Testing Information Extraction with Sample Chats ---\n",
            "\n",
            "--- Sample Chat 1 ---\n",
            "Customer: Hi, I'm having trouble with my order\n",
            "Agent: I'd be happy to help! Can I get your name and email?\n",
            "Customer: Sure, it's Sarah Johnson and my email is sarah.johnson@email.com\n",
            "Agent: Thank you Sarah. Can I also get your phone number and address?\n",
            "Customer: Yes, my phone is (555) 123-4567 and I live in Seattle, WA. I'm 28 years old by the way, does that matter for shipping?\n",
            "Agent: Thank you for all that information! Just to confirm, what tech do you use the most?\n",
            "Customer: Mostly Python and Django for my small business website.\n",
            "Agent: Let me look up your order.\n",
            "\n",
            "‚úÖ Extracted and Validated Information:\n",
            "{\n",
            "  \"name\": \"Sarah Johnson\",\n",
            "  \"email\": \"sarah.johnson@email.com\",\n",
            "  \"phone\": \"(555) 123-4567\",\n",
            "  \"location\": \"Seattle, WA\",\n",
            "  \"age\": 28,\n",
            "  \"tech_stack\": [\n",
            "    \"Python\",\n",
            "    \"Django\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "--- Sample Chat 2 ---\n",
            "User: I want to sign up for your newsletter\n",
            "Bot: Great! What's your email address?\n",
            "User: It's mike.chen@company.com\n",
            "Bot: Thanks! And your name?\n",
            "User: Mike Chen\n",
            "Bot: Any location preference for local events?\n",
            "User: I'm based in San Francisco.\n",
            "Bot: Do you use any specific tech tools or frameworks at work?\n",
            "User: Node.js and React.\n",
            "Bot: Perfect, you're all set!\n",
            "\n",
            "‚úÖ Extracted and Validated Information:\n",
            "{\n",
            "  \"name\": \"Mike Chen\",\n",
            "  \"email\": \"mike.chen@company.com\",\n",
            "  \"phone\": null,\n",
            "  \"location\": \"San Francisco\",\n",
            "  \"age\": null,\n",
            "  \"tech_stack\": [\n",
            "    \"Node.js\",\n",
            "    \"React\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "--- Sample Chat 3 ---\n",
            "Interviewer: Thank you for participating in our survey. Could you share some basic information?\n",
            "Participant: Of course\n",
            "Interviewer: What's your name and age?\n",
            "Participant: I'm Jennifer Davis, 35 years old\n",
            "Interviewer: Great! Do you have a preferred contact method?\n",
            "Participant: You can reach me at jennifer.davis.work@gmail.com or call me at 555-987-6543\n",
            "Interviewer: And which city are you located in?\n",
            "Participant: I live in Austin, Texas\n",
            "Interviewer: Do you work in any particular technology area?\n",
            "Participant: Yes, mainly with AWS, Java, and Docker.\n",
            "Interviewer: Perfect, that's all we need!\n",
            "\n",
            "‚úÖ Extracted and Validated Information:\n",
            "{\n",
            "  \"name\": \"Jennifer Davis\",\n",
            "  \"email\": \"jennifer.davis.work@gmail.com\",\n",
            "  \"phone\": \"555-987-6543\",\n",
            "  \"location\": \"Austin, Texas\",\n",
            "  \"age\": 35,\n",
            "  \"tech_stack\": [\n",
            "    \"AWS\",\n",
            "    \"Java\",\n",
            "    \"Docker\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "--- Sample Chat 4 ---\n",
            "Recruiter: Welcome, could you please tell me your name and how to reach you?\n",
            "Candidate: My name is Priya Sharma. You can email me at priya.sharma123@gmail.com.\n",
            "Recruiter: And your phone number and current location?\n",
            "Candidate: Sure, my phone is +91-9876543210, and I'm currently in Bengaluru.\n",
            "Recruiter: What's your age and primary tech stacks?\n",
            "Candidate: I'm 25. I primarily work with Vue.js, FastAPI, and PostgreSQL.\n",
            "Recruiter: Noted. Do you have experience with cloud platforms?\n",
            "Candidate: Yes, GCP and Azure are part of my daily workflow.\n",
            "\n",
            "‚úÖ Extracted and Validated Information:\n",
            "{\n",
            "  \"name\": \"Priya Sharma\",\n",
            "  \"email\": \"priya.sharma123@gmail.com\",\n",
            "  \"phone\": \"9876543210\",\n",
            "  \"location\": \"Bengaluru\",\n",
            "  \"age\": 25,\n",
            "  \"tech_stack\": [\n",
            "    \"Vue.js\",\n",
            "    \"FastAPI\",\n",
            "    \"PostgreSQL\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "--- Sample Chat 5 ---\n",
            "Mentor: Hi! Welcome aboard. What's your full name?\n",
            "Newcomer: I'm Alex Martinez.\n",
            "Mentor: Great, Alex. You can share your contact details when you're ready.\n",
            "Newcomer: Sure! My email: alexmartinez@startup.io\n",
            "Mentor: Where are you joining from?\n",
            "Newcomer: I'm based in Boston. I usually build with Flutter and Firebase.\n",
            "Mentor: Awesome! Age is optional, but it helps with mentorship pairing.\n",
            "Newcomer: I'm 30.\n",
            "Mentor: Thanks, Alex. Let's start!\n",
            "\n",
            "‚úÖ Extracted and Validated Information:\n",
            "{\n",
            "  \"name\": \"Alex Martinez\",\n",
            "  \"email\": \"alexmartinez@startup.io\",\n",
            "  \"phone\": null,\n",
            "  \"location\": \"Boston\",\n",
            "  \"age\": 30,\n",
            "  \"tech_stack\": [\n",
            "    \"Flutter\",\n",
            "    \"Firebase\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "--- Information Extraction Test Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VghoFaK9pgXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}