{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9gPRrq8PGjJb",
        "1ctXTVPmP8Fq",
        "6uTVD5n3eVyy",
        "f7t-lgDqeZq2",
        "s4Cge2reedr-",
        "3Yo043fbK0z5",
        "IS6xIFOLZLc0",
        "p5cdHC6JfLDo",
        "k5dMfgn5nmpT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloads and Imports**"
      ],
      "metadata": {
        "id": "9gPRrq8PGjJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD0QavPNFhcp"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet requests openai jsonschema pydantic[email]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from typing import List, Dict, Literal"
      ],
      "metadata": {
        "id": "bXXbMbfVH7BS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Configurations and Model Instances**"
      ],
      "metadata": {
        "id": "b4_fU74jJ66f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHAT_MODEL = \"llama-3.3-70b-versatile\"\n",
        "STRUCTURED_OUTPUT_MODEL = \"openai/gpt-oss-20b\"\n"
      ],
      "metadata": {
        "id": "C0MEJ6f1LM4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key = \"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\"\n",
        ")\n",
        "# Kept API key for testing, might revoke in 1 week"
      ],
      "metadata": {
        "id": "AyNdxjW7J-Dt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: Managing Conversation History with Summarization**\n",
        "\n",
        "- Message History Tracking\n",
        "- Summarization\n",
        "- Truncation"
      ],
      "metadata": {
        "id": "1ctXTVPmP8Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MESSAGE_HISTORY : List[dict] = []\n",
        "SUMMARY = \"\""
      ],
      "metadata": {
        "id": "fUIAOVyVqQ90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_interval = 8\n",
        "turn_count = 0\n",
        "# summarization_interval,  The interval / turn count after which history summarization will happen\n",
        "# turn_count  Variable to track turn"
      ],
      "metadata": {
        "id": "H1JITagtEiCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to add messages"
      ],
      "metadata": {
        "id": "6uTVD5n3eVyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_message(message:dict):\n",
        "    global turn_count\n",
        "    # Add message to history and increment turn\n",
        "    MESSAGE_HISTORY.append(message)\n",
        "    turn_count += 1\n",
        "\n",
        "    print(f\"Turn {turn_count}: Added {message}\")\n",
        "\n",
        "    # Check if periodic summarization is needed\n",
        "    if turn_count % summarization_interval == 0:\n",
        "      print(f\"Turn count {turn_count}, output: {turn_count % summarization_interval}\")\n",
        "      periodic_summarization()"
      ],
      "metadata": {
        "id": "ydfk_B0OP_br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## functionm to create summary"
      ],
      "metadata": {
        "id": "f7t-lgDqeZq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(messages):\n",
        "\n",
        "    if not MESSAGE_HISTORY:\n",
        "        print(\"No messages to summarize.\")\n",
        "        return\n",
        "\n",
        "    # Convert messsage history which is in dictionary format, to a string\n",
        "    conversation_text = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in MESSAGE_HISTORY])\n",
        "\n",
        "    summary_prompt = f\"\"\"\n",
        "        Please provide a concise summary of the following conversation, in the third person point of view. Focus on:\n",
        "        1. Main topics discussed\n",
        "        2. Key decisions or outcomes\n",
        "        3. Important information exchanged\n",
        "        4. Overall context and flow\n",
        "\n",
        "        Conversation:\n",
        "        {conversation_text}\n",
        "\n",
        "        Summary:\"\"\"\n",
        "    try:\n",
        "        chat_bot_response = client.chat.completions.create(\n",
        "          model=CHAT_MODEL,\n",
        "            messages=[{\"role\":\"user\", \"content\":conversation_text}],\n",
        "            max_tokens=150,\n",
        "            temperature=0.3)\n",
        "\n",
        "        summary = chat_bot_response.choices[0].message.content.strip()\n",
        "\n",
        "        print(f\"✅ Generated summary ({len(summary)} characters)\")\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generating summary: {e}\")\n",
        "        return \"Summary generation failed\""
      ],
      "metadata": {
        "id": "P_io4vKcWO6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to create periodic summary"
      ],
      "metadata": {
        "id": "s4Cge2reedr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def periodic_summarization():\n",
        "        global MESSAGE_HISTORY\n",
        "        global SUMMARY\n",
        "\n",
        "        # Only generate a new summary if there's enough new conversation\n",
        "        if len(MESSAGE_HISTORY) >= 2:\n",
        "            current_summary = generate_summary(MESSAGE_HISTORY)\n",
        "\n",
        "            if SUMMARY:\n",
        "                # Combine with previous summary\n",
        "                combined_prompt = f\"\"\"\n",
        "                Previous summary: {SUMMARY}\n",
        "\n",
        "                New conversation summary: {current_summary}\n",
        "\n",
        "                Please create a comprehensive summary that combines both:\"\"\"\n",
        "\n",
        "                try:\n",
        "                    response = client.chat.completions.create(\n",
        "                        model=CHAT_MODEL,\n",
        "                        messages=[{\"role\": \"user\", \"content\": combined_prompt}],\n",
        "                        max_tokens= 300,\n",
        "                        temperature=0.3\n",
        "                    )\n",
        "\n",
        "                    SUMMARY = response.choices[0].message.content.strip()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Error combining summaries: {e}\")\n",
        "                    SUMMARY = current_summary\n",
        "            else:\n",
        "                SUMMARY = current_summary\n",
        "\n",
        "            # Replace history with summary\n",
        "            MESSAGE_HISTORY = [\n",
        "                {\"role\": \"system\", \"content\": f\"Previous conversation summary: {SUMMARY}\"}\n",
        "            ]\n",
        "\n",
        "            print(f\"Conversation summarized and history reset\")\n",
        "            print(f\"Current summary length: {len(SUMMARY)} characters\")"
      ],
      "metadata": {
        "id": "kkFgwJ8XezA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "TxuCIaXwGg1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_words(text: str) -> int:\n",
        "    return len(text.split())\n",
        "\n",
        "def count_characters(messages: List[Dict]) -> int:\n",
        "    return sum(len(msg[\"content\"]) for msg in messages)\n",
        "\n",
        "def count_total_words(messages: List[Dict]) -> int:\n",
        "    return sum(count_words(msg[\"content\"]) for msg in messages)"
      ],
      "metadata": {
        "id": "LIu7myvAGgVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to truncate conversation by turns"
      ],
      "metadata": {
        "id": "tnvwOw6CG5i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_turns, number of messages after which we want to truncate\n",
        "def truncate_by_turns(messages: List[Dict], max_turns:int) -> List[Dict]:\n",
        "  if max_turns and len(messages) > max_turns:\n",
        "      truncated = messages[-max_turns:]\n",
        "      print(f\"Truncated to last {max_turns} messages\")\n",
        "      return truncated\n",
        "  return messages"
      ],
      "metadata": {
        "id": "pE0WOBZKHAua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Truncate by length"
      ],
      "metadata": {
        "id": "K3yTmfOHID7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# max_characters\n",
        "# max_words\n",
        "def truncate_by_length(messages: List[Dict], max_characters:int, max_words:int) -> List[Dict]:\n",
        "    if not (max_characters or max_words):\n",
        "        return messages\n",
        "\n",
        "    result = []\n",
        "    current_chars = 0\n",
        "    current_words = 0\n",
        "\n",
        "    # Start from the end to keep most recent messages\n",
        "    for message in reversed(messages):\n",
        "        msg_chars = len(message[\"content\"])\n",
        "        msg_words = count_words(message[\"content\"])\n",
        "\n",
        "        # Check if adding this message would exceed limits\n",
        "        if (max_characters and current_chars + msg_chars > max_characters) or (max_words and current_words + msg_words > max_words):\n",
        "            break\n",
        "\n",
        "        result.insert(0, message)\n",
        "        current_chars += msg_chars\n",
        "        current_words += msg_words\n",
        "\n",
        "    if len(result) < len(messages):\n",
        "        removed = len(messages) - len(result)\n",
        "        print(f\"Truncated {removed} messages.\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "7bbJJTIcIGaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test Conversation Manager**"
      ],
      "metadata": {
        "id": "p4pUiT_Sev_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Conversation with LLM, test summarization"
      ],
      "metadata": {
        "id": "3Yo043fbK0z5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_interval = 3\n",
        "# turn_count = 0"
      ],
      "metadata": {
        "id": "6AG9JP64E0GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start chat with LLM\n",
        "turn_count = 0 # reset turn count\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    add_message({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    try:\n",
        "        chat_bot_response = client.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=MESSAGE_HISTORY\n",
        "        )\n",
        "        response_content = chat_bot_response.choices[0].message.content\n",
        "        add_message({\"role\": \"assistant\", \"content\": response_content})\n",
        "\n",
        "        print(f\"Bot: {response_content}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "kc8J6W-8R7-g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "8838f524-f602-4e4e-d8a2-c0f7b32edb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: HI\n",
            "Turn 1: Added {'role': 'user', 'content': 'HI'}\n",
            "An error occurred: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k51jay80fms9rn3d2vz9sj69` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 78438, Requested 60648. Please try again in 9h22m50.062s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1035460742.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mturn_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# reset turn count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0madd_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, summary has been generated after 8 turns, and previous is added (if any).\n",
        "Below is generated summary."
      ],
      "metadata": {
        "id": "4x0gq04fIlid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SUMMARY"
      ],
      "metadata": {
        "id": "pbhqZoMjAM9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test truncation with test message jsons"
      ],
      "metadata": {
        "id": "hd7l3d_nK4Rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_samples = [\n",
        "    (\"user\", \"Hi! I need advice on starting a food truck business.\"),\n",
        "    (\"assistant\", \"That's exciting! What type of cuisine are you considering for your food truck?\"),\n",
        "    (\"user\", \"I'm interested in serving gourmet tacos and fresh juices.\"),\n",
        "    (\"assistant\", \"Great choice! Have you picked a location or checked local food truck regulations?\"),\n",
        "    (\"user\", \"Not yet. Do you have tips on selecting a good spot?\"),\n",
        "    (\"assistant\", \"Absolutely. Look for areas with heavy foot traffic, local events, and nearby office parks.\"),\n",
        "    (\"user\", \"Thanks! What licenses do I need to get started?\"),\n",
        "    (\"assistant\", \"You'll need a mobile food vendor license, health permits, and possibly parking permits depending on your city.\"),\n",
        "    (\"user\", \"Got it. What about marketing ideas for my launch?\"),\n",
        "    (\"assistant\", \"Consider social media campaigns, local food festivals, and offering promo deals for first-time customers.\"),\n",
        "    (\"user\", \"That's helpful. How much investment do I need to get started?\"),\n",
        "    (\"assistant\", \"Typically, starting a food truck costs between $50,000 and $150,000, depending on equipment, customization, and permits.\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "Sv4vAGdYK_e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear previous history for a clean test\n",
        "MESSAGE_HISTORY = []\n",
        "turn_count = 0\n",
        "SUMMARY = \"\"\n",
        "\n",
        "print(\"-------- Testing Truncation --------\")\n",
        "\n",
        "# fill message history with sample conversation\n",
        "for role, content in conversation_samples:\n",
        "    add_message({\"role\": role, \"content\": content})\n",
        "\n",
        "print(\"\\n-------- Original Message History --------\")\n",
        "for msg in MESSAGE_HISTORY:\n",
        "    print(f\"{msg['role']}: {msg['content']}\")\n",
        "print(f\"Total messages: {len(MESSAGE_HISTORY)}\")\n",
        "\n",
        "print(\"\\n-------- Truncating by Turns (max_turns=3) --------\")\n",
        "truncated_by_turns = truncate_by_turns(MESSAGE_HISTORY, max_turns=3)\n",
        "for msg in truncated_by_turns:\n",
        "    print(f\"{msg['role']}: {msg['content']}\")\n",
        "print(f\"Truncated messages: {len(truncated_by_turns)}\")\n",
        "\n",
        "\n",
        "print(\"\\n-------- Truncating by Length (max_characters=300, max_words=80) --------\")\n",
        "truncated_by_length = truncate_by_length(MESSAGE_HISTORY, max_characters=300, max_words=80)\n",
        "for msg in truncated_by_length:\n",
        "    print(f\"{msg['role']}: {msg['content']}\")\n",
        "print(f\"Truncated messages: {len(truncated_by_length)}\")\n",
        "\n",
        "print(\"\\n-------- Truncation Test Complete --------\")"
      ],
      "metadata": {
        "id": "E880Q_grLBj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: JSON Schema Classification & Information Extraction**"
      ],
      "metadata": {
        "id": "CvA01nyZYASu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required JSON Schema and Prompts"
      ],
      "metadata": {
        "id": "IS6xIFOLZLc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, EmailStr\n",
        "from typing import List, Dict, Literal, Optional\n",
        "\n",
        "class UserInformationModel(BaseModel):\n",
        "    name: Optional[str] = None\n",
        "    email: Optional[EmailStr] = None\n",
        "    phone: Optional[str] = None\n",
        "    location: Optional[str] = None\n",
        "    age: Optional[int] = None\n",
        "    tech_stack: Optional[List[str]] = None"
      ],
      "metadata": {
        "id": "L8FwfVZBYGWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = system_prompt = \"\"\"You are an expert information extractor. Your task is to carefully analyze chat conversations and extract specific user information when available.\n",
        "\n",
        "\n",
        "FOLLOW THESE IMPORTANT GUIDELINES:\n",
        "1. Extract only information that is explicitly mentioned in the conversation\n",
        "2. Do not make assumptions or infer information not directly stated\n",
        "3. For names: Extract full names when provided\n",
        "4. For emails: Look for email addresses in standard formats, should contain '@'\n",
        "5. For phones: Extract any phone number mentioned (any format, should be 10 digit)\n",
        "6. For locations: Extract cities, states, addresses, or geographical references\n",
        "7. For age: Extract numerical age when mentioned\n",
        "8. For tech_stack: Extract a list of technical skills when mentioned\n",
        "\n",
        "Cross check all information before giving an output.\n",
        "\n",
        "If information is not clearly given, leave that field empty instead of guessing.\"\"\""
      ],
      "metadata": {
        "id": "l74lmwFgYdLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to extract information"
      ],
      "metadata": {
        "id": "p5cdHC6JfLDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_information(chat_conversation: str | List[Dict]) -> dict:\n",
        "\n",
        "  USER_PROMPT = f\"\"\"Please extract any available user information from this chat conversation:\n",
        "\n",
        "{chat_conversation}\n",
        "\n",
        "Use the extract_user_information function to provide the structured output.\"\"\"\n",
        "  try:\n",
        "      chat_bot_response = client.chat.completions.create(\n",
        "            model=STRUCTURED_OUTPUT_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\":\"user\", \"content\":USER_PROMPT},\n",
        "            ],\n",
        "            response_format =  {\n",
        "                \"type\": \"json_schema\",\n",
        "                \"json_schema\":{\n",
        "                    \"name\":\"user_information\",\n",
        "                    \"description\":\"User information extracted from the chat conversation\",\n",
        "                    \"schema\":UserInformationModel.model_json_schema()\n",
        "\n",
        "                } }\n",
        "        )\n",
        "      return chat_bot_response.choices[0].message.content\n",
        "  except Exception as e:\n",
        "    raise Exception(f\"Error extracting information: {e}\")"
      ],
      "metadata": {
        "id": "xt8sIJ5PfPEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Information Extraction"
      ],
      "metadata": {
        "id": "k5dMfgn5nmpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_CHATS = [\n",
        "    # Sample 1: Customer support chat with complete information, including tech stack\n",
        "    \"\"\"Customer: Hi, I'm having trouble with my order\n",
        "Agent: I'd be happy to help! Can I get your name and email?\n",
        "Customer: Sure, it's Sarah Johnson and my email is sarah.johnson@email.com\n",
        "Agent: Thank you Sarah. Can I also get your phone number and address?\n",
        "Customer: Yes, my phone is (555) 123-4567 and I live in Seattle, WA. I'm 28 years old by the way, does that matter for shipping?\n",
        "Agent: Thank you for all that information! Just to confirm, what tech do you use the most?\n",
        "Customer: Mostly Python and Django for my small business website.\n",
        "Agent: Let me look up your order.\"\"\",\n",
        "\n",
        "    # Sample 2: Registration chat with partial information & tech stack\n",
        "    \"\"\"User: I want to sign up for your newsletter\n",
        "Bot: Great! What's your email address?\n",
        "User: It's mike.chen@company.com\n",
        "Bot: Thanks! And your name?\n",
        "User: Mike Chen\n",
        "Bot: Any location preference for local events?\n",
        "User: I'm based in San Francisco.\n",
        "Bot: Do you use any specific tech tools or frameworks at work?\n",
        "User: Node.js and React.\n",
        "Bot: Perfect, you're all set!\"\"\",\n",
        "\n",
        "    # Sample 3: Survey chat with mixed information\n",
        "    \"\"\"Interviewer: Thank you for participating in our survey. Could you share some basic information?\n",
        "Participant: Of course\n",
        "Interviewer: What's your name and age?\n",
        "Participant: I'm Jennifer Davis, 35 years old\n",
        "Interviewer: Great! Do you have a preferred contact method?\n",
        "Participant: You can reach me at jennifer.davis.work@gmail.com or call me at 555-987-6543\n",
        "Interviewer: And which city are you located in?\n",
        "Participant: I live in Austin, Texas\n",
        "Interviewer: Do you work in any particular technology area?\n",
        "Participant: Yes, mainly with AWS, Java, and Docker.\n",
        "Interviewer: Perfect, that's all we need!\"\"\",\n",
        "\n",
        "    # Sample 4: Job application chat with multiple tech skills\n",
        "    \"\"\"Recruiter: Welcome, could you please tell me your name and how to reach you?\n",
        "Candidate: My name is Priya Sharma. You can email me at priya.sharma123@gmail.com.\n",
        "Recruiter: And your phone number and current location?\n",
        "Candidate: Sure, my phone is +91-9876543210, and I'm currently in Bengaluru.\n",
        "Recruiter: What's your age and primary tech stacks?\n",
        "Candidate: I'm 25. I primarily work with Vue.js, FastAPI, and PostgreSQL.\n",
        "Recruiter: Noted. Do you have experience with cloud platforms?\n",
        "Candidate: Yes, GCP and Azure are part of my daily workflow.\"\"\",\n",
        "\n",
        "    # Sample 5: Casual onboarding chat with partial information\n",
        "    \"\"\"Mentor: Hi! Welcome aboard. What's your full name?\n",
        "Newcomer: I'm Alex Martinez.\n",
        "Mentor: Great, Alex. You can share your contact details when you're ready.\n",
        "Newcomer: Sure! My email: alexmartinez@startup.io\n",
        "Mentor: Where are you joining from?\n",
        "Newcomer: I'm based in Boston. I usually build with Flutter and Firebase.\n",
        "Mentor: Awesome! Age is optional, but it helps with mentorship pairing.\n",
        "Newcomer: I'm 30.\n",
        "Mentor: Thanks, Alex. Let's start!\"\"\"\n",
        "]"
      ],
      "metadata": {
        "id": "pBTLdEbLgj1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Testing Information Extraction with Sample Chats ---\")\n",
        "\n",
        "for i, chat in enumerate(SAMPLE_CHATS):\n",
        "    print(f\"\\n--- Sample Chat {i+1} ---\")\n",
        "    print(chat)\n",
        "    try:\n",
        "        extracted_data_json = extract_information(chat)\n",
        "\n",
        "        extracted_data = json.loads(extracted_data_json)\n",
        "        validated_data = UserInformationModel.model_validate(extracted_data)\n",
        "        print(\"\\n✅ Extracted and Validated Information:\")\n",
        "        print(validated_data.model_dump_json(indent=2))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Error processing sample chat {i+1}: {e}\")\n",
        "\n",
        "print(\"\\n--- Information Extraction Test Complete ---\")"
      ],
      "metadata": {
        "id": "IhF8SSq5nWI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VghoFaK9pgXF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}